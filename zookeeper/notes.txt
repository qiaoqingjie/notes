The deafult weight of a server is 1
[3.4.5:
quorumVerifier:
	1\serverGroup.size() > 0：QuorumHierarchical：包含半数以上group（numGroups不包含weight为0的group），并且其中每个group的总权值大于配置的group的总权值的一半。
	2\QuorumMaj：包含半数以上类型为PARTICIPANT的server数（half = votingMembers.size() / 2;）
]

[trunk:
配置文件中有key为group或weight：QuorumHierarchical
否则：QuorumMaj

]

[
zoo.cfg:
	...
	dynamicConfigFile
	
dynamicConfigFile：
	group（必须要包含所有PARTICIPANT类型的server）
	weight
	server.（这里的value（host:port:electionPort[:type]）中可以配置clientAddr（port或host:port，没有host则用server的value中的host），用;与前面的addr隔开。(若zoo.cfg中配置了，则两者必须一致，否则报错））
	version（QuorumVerifier的version）
]	


[cfg


]

[dynamicConfigFile

group.1=1:2:3
group.2=4:5:6

weight.1=100
weight.2=50

server.1=host:port:port
server.1=host:port:port:type
server.1=host:port:port:type;port
server.1=host:port:port:type;host:port

version=0

格式：
server.sid=host1:port1:port2[:type][;[host2:]port3]
	type取值:observer participant
	分号后的host没有，就用前面的host

group.gid=sid(:sid)*
weight.sid=n
	sid gid n 都是long类型
	对participant类型的所有server，必须包含在group中（否则报错）；如果没配置weight，默认为1。


QuorumServer.addr	host1:port1
QuorumServer.electionAddr	host1:port2
QuorumServer.clientAddr	(host2 | host1):port3

]

[.next
如果cfg中有dynamicConfigFile，并且File nextDynamicConfigFile = new File(dynamicConfigFileStr + ".next"); nextDynamicConfigFile.exists()，则lastSeenQuorumVerifier = createQuorumVerifier(dynamicConfigNextCfg, isHierarchical); 
]




[server类型:
public enum LearnerType {
        PARTICIPANT, OBSERVER;
    }
]
[public enum ServerState {
        LOOKING, FOLLOWING, LEADING, OBSERVING;
    }
]

[?????????????????????????
dynamicConfigFileStr + ".next" (lastSeenQuorumVerifier是干嘛的？)
]


[
ServerCnxnFactory
	NettyServerCnxnFactory
	NIOServerCnxnFactory
	NullServerCnxnFactory
]
[protected int maxClientCnxns = 60;]

[PathTrie
TrieNode:
PathTrie:每级对应一个节点
	addPath:将最后一个节点property设为true。
	deletePath：如果要删除的path最后一个节点中children只有一个，则删除该节点（及其子节点）；否则，只设置最后一个节点property为false。
	findMaxPrefix：找到与path重合的路径中，最长的路径，使得最后一个节点的property为true。
]

[??????????????!!!!!!!!!!!!!!!!!!!!!
DataNode
void copyStat(Stat to)

// when we do the Cversion we need to translate from the count of the creates
        // to the count of the changes (v3 semantics)
        // for every create there is a delete except for the children still present
        to.setCversion(stat.getCversion()*2 - numChildren);
ps:DataNode中的StatPersisted stat中的cversion只是create的计数，不包含delete；copyStat中，to的cversion包含了delete的计数，转换方法就是上面的。
]

[??????????????????
Util.isValidSnapshot
RandomAccessFile读取最后n个byte，可能会出现只读取到少于n个byte的情况？？？
]

[???????????????????????！！！！！！！！！！！！
FileTxnLog.getLogFiles(File[] logDirList,long snapshotZxid)为啥实现的那么复杂，用>=不久完事了吗？（返回>=snapshotZxid的文件）
！！！！！！！！！！！Aug25
原因：
目标是，Find the log file that starts at, or just before, the snapshot。
logDirList中如果有snapshotZxid，则返回>=snapshotZxid的所有文件；
如果没有，需要返回>snapshotZxid的所有文件，和，<snapshotZxid的最大的文件。

源码：
public static File[] getLogFiles(File[] logDirList,long snapshotZxid) {
	List<File> files = Util.sortDataDir(logDirList, "log", true);
	long logZxid = 0;
	// Find the log file that starts before or at the same time as the
	// zxid of the snapshot
	for (File f : files) {
	    long fzxid = Util.getZxidFromName(f.getName(), "log");
	    if (fzxid > snapshotZxid) {
		continue;
	    }
	    // the files
	    // are sorted with zxid's
	    if (fzxid > logZxid) {
		logZxid = fzxid;
	    }
	}
	List<File> v=new ArrayList<File>(5);
	for (File f : files) {
	    long fzxid = Util.getZxidFromName(f.getName(), "log");
	    if (fzxid < logZxid) {
		continue;
	    }
	    v.add(f);
	}
	return v.toArray(new File[0]);

}

]

[
对DataNode n的操作(读写)，全部用synchronized(n){...}
]


[??????????????????????????
/**
         * this is to avoid the jvm bug:
         * NullPointerException in Selector.open()
         * http://bugs.sun.com/view_bug.do?bug_id=6427854
         */
        try {
            Selector.open().close();
        } catch(IOException ie) {
            LOG.error("Selector failed to open", ie);
        }

]


[toooooooooooooooooooooooo
ConnectionExpirerThread
ExpiryQueue
]



[FLE
任意两个节点间有且只有一个socket连接，都是sid小的做server，sid大的做client。


]


[???????????????????????????????????????????
QuorumConnectionManager.receiveConnection(Socket sock)中：
int num_read = din.read(b);
有问题：din.read(b)读取字节数不确定吧？咋不用readFully或者while(r = din.read(b) != -1)?
if (num_read == num_remaining_bytes) {


===================
QuorumConnectionManager.RecvWorker.run()中：
	byte[] msgArray = new byte[length];
        din.readFully(msgArray, 0, length);
        ByteBuffer message = ByteBuffer.wrap(msgArray);
]


[??????????????????????????????????????????
QuorumConnectionManager.SendWorker.send(ByteBuffer b):
 	try {
                b.position(0);
                b.get(msgBytes);
            } catch (BufferUnderflowException be) {
                LOG.error("BufferUnderflowException ", be);
                return;
            }	
有啥用？？？？？？？？？？
]

[QuorumCnxManager
ConcurrentHashMap<Long, ArrayBlockingQueue<ByteBuffer>> queueSendMap 
一个thread放(FastLeaderElection.WorkerSender从sendqueue中取，放入manager的queueSendMap（QuorumConnectionManager.toSend(Long sid, ByteBuffer b)）)，
一个thread取(每个sid对应一个QuorumConnectionManager.SendWorker)
所以ArrayBlockingQueue<ByteBuffer>不需要lock
]

[QuorumCnxManager
ArrayBlockingQueue<Message> recvQueue
放：
	FastLeaderElection.Messenger.WorkerSender，如果发给自己，放入recvQueue；
	QuorumCnxManager.RecvWorker，放入recvQueue；（自己和每个quorum peer都有一个socket，对应一个RecvWorker)
取：
]





[QuorumCnxManager: a connection manager for leader election using TCP

	ArrayBlockingQueue<Message> recvQueue;
	ConcurrentHashMap<Long, ArrayBlockingQueue<ByteBuffer>> queueSendMap;

QuorumCnxManager.Listener:(extends Thread)
	ServerSocket监听electinAddr，when accept a socket, read protocolVersion/sid/electionAddr(host:port) from it, if sid < self.getId(), close it and call connectOne(sid, electionAddr);
		else, create 2 threads, SendWorker and RecvWorker and start them(before start, close the old 2 threads if exist). 

QuorumCnxManager.SendWorker:
	从ConcurrentHashMap<Long, ArrayBlockingQueue<ByteBuffer>> queueSendMap找到ArrayBlockingQueue<ByteBuffer>(SEND_CAPACITY是1,why?????????)，poll出ByteBuffer，lastMessageSent.put(sid, b)，写入socket。
QuorumCnxManager.RecvWorker:
	从socket阻塞读数据，ByteBuffer message = ByteBuffer.wrap(msgArray)，new Message(message.duplicate(), sid)，addToRecvQueue(message)

QuorumCnxManager.toSend(Long sid, ByteBuffer b):
	如果self.getId() == sid，addToRecvQueue(new Message(b.duplicate(), sid));
	如果queueSendMap.containsKey(sid)，ArrayBlockingQueue<ByteBuffer> bq = queueSendMap.get(sid)，否则新建一个new ArrayBlockingQueue<ByteBuffer>(SEND_CAPACITY)并放入queueSendMap，addToSendQueue(bq, b);
	

QuorumCnxManager.addToRecvQueue(Message msg)：(recvQueue capacity 是100)
	在recvQLock同步下，如果recvQueue.remainingCapacity() == 0，就recvQueue.remove()，然后recvQueue.add(msg)。
QuorumCnxManager.addToSendQueue(ArrayBlockingQueue<ByteBuffer> queue,ByteBuffer buffer):(SEND_CAPACITY是1）(why1????????????????????????????????????????????????)
	如果queue.remainingCapacity() == 0,就queue.remove();然后queue.add(buffer);
]



[FastLeaderElection

FastLeaderElection.Messenger:
	启动WorkerSender和WorkerReceiver两个thread
FastLeaderElection.Messenger.WorkerSender：
	ToSend m = sendqueue.poll(3000, TimeUnit.MILLISECONDS), 构造ByteBuffer，manager.toSend(m.sid, requestBuffer);
FastLeaderElection.Messenger.WorkerReceiver:
	Message response = manager.pollRecvQueue(3000, TimeUnit.MILLISECONDS);
	如果!self.getVotingView().containsKey(response.sid)，
		则将自己当前所认为的leader（self.getCurrentVote()）立即发过去（sendqueue.offer(notmsg)）；
	否则，
		如果self.getPeerState() == QuorumPeer.ServerState.LOOKING，
			则recvqueue.offer(n);
			如果对方在looking，并且对方的electionEpoch < 我的logicalclock，
				则将proposedLeader, proposedZxid, proposedEpoch发过去。（sendqueue.offer(notmsg)）
		否则，
			如果对方在looking，
				则将自己当前所认为的leader（self.getCurrentVote()）立即发过去（sendqueue.offer(notmsg)）；
]

[????????????????????
FLE咋感觉不是类paxos呢？？？？？木有paxos的特征？？？？？淘宝的人居然说FLE是fast paxos？？？？？？

最近又思考了下，好像有点像paxos。Aug20
]

[FastLeaderElection.lookForLeader()
	HashMap<Long, Vote> recvset = new HashMap<Long, Vote>();
	HashMap<Long, Vote> outofelection = new HashMap<Long, Vote>();
	synchronized(this){
                logicalclock++;
                updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());
        }
	sendNotifications();
	.............
]

[toooooooooooooooooooooooooooooooooooooooooo0000000000
QuorumPeer.run()
	readonlymode.enabled


]



[
lastProcessedZxid <= currentEpoch <= acceptedEpoch
后两个各自写入同名文件中，启动时从文件中读，如果没有，设置为lastProcessedZxid对应的值，并写入文件。
]

[
选举用sid lastProcessedZxid currentEpoch，只读，不会对其值修改。

Phase 1: Establish an epoch中，用acceptedEpoch
	leader选出半数以上follower的最大的acceptedEpoch为新的epoch；在等到半数以上ack后，设置currentEpoch为epoch。
	follower:设置acceptedEpoch为上面的epoch。



]

[???????????????????????????????????????????????????????????????????哪里设置了lastProcessedZxid？？？？？？？？！！！！！！！！！！！！！！！！
LearnerHandler
public boolean syncFollower(long peerLastZxid, ZKDatabase db, Leader leader) {
        /*
         * When leader election is completed, the leader will set its
         * lastProcessedZxid to be (epoch < 32). There will be no txn associated
         * with this zxid.
         *

！！！！！！！！！！！！！！！！！！！！！！！
follower在接收到leader的opPacket（DIFF TRUNK SNAP）并完成后，zk.getZKDatabase().setlastProcessedZxid(qp.getZxid());
]


[tooooooooooooooooooooooooooooooo

FileTxnIterator 彻底总结！！！！！


]

