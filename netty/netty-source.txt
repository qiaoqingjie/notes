看netty source的目的：完全搞清楚netty的运行过程细节，更好的使用netty、快速排错。

1、Bootstrap。
DefaultChannelPipeline 包含DefaultChannelHandlerContext的链表（head、tail）；

DefaultChannelHandlerContext包含prev、next、name、ChannelHandler（ChannelUpstreamHandler或ChannelDownstreamHandler）。

2、ServerBootstrap。

pipeline和channel是一对一的。pipeline和sink也是一对一。
（DefaultChannelPipeline中有一个Channel、一个ChannelSink）

（一个channel对应一个worker（一个worker对应多个channel），所以channel、pipeline、handler、framedecoder是线程安全的？？！）



 channel.getPipeline().sendUpstream(
                new DefaultExceptionEvent(channel, cause))
从DefaultChannelPipeline 的head开始，找到第一个可以canHandleUpstream的DefaultChannelHandlerContext，然后((ChannelUpstreamHandler) ctx.getHandler()).handleUpstream(ctx, e);

DefaultChannelPipeline的sendDownstream(ChannelEvent e)从tail开始，找到第一个canHandleDownstream（）的DefaultChannelHandlerContext，然后((ChannelDownstreamHandler) ctx.getHandler()).handleDownstream(ctx, e);，如果找不到（null），getSink().eventSunk(this, e);


 DefaultChannelPipeline.attach(Channel channel, ChannelSink sink) 只能一次 



????????????????????????????????????????????????????????????????????
 if (CONSTRAINT_LEVEL != 0) {
                selectorGuard.writeLock().lock();
                    // This empty synchronization block prevents the selector
                    // from acquiring its lock.
                selectorGuard.writeLock().unlock();
            }
??????????????????????????????????????????????????????????????????????


 DefaultChannelPipeline.sendDownstream(ChannelEvent e)中，if (tail == null)则getSink().eventSunk(this, e);


读取数据，用的buffer是DirectByteBuffer，重用，size是1024的倍数，size不满足等情况，先销毁再创建新的，销毁用反射，因为DirectByteBuffer和其中的cleaner是不可见的。（class.forName（）、method.setAccessible(true)）；读完后，复制到BigEndianHeapChannelBuffer中，最后fireMessageReceived(channel, buffer)。


================ChannelBuffer======================================================================================		
							改变自己的writerIndex	改变参数的writerIndex
writeBytes(ChannelBuffer src, int srcIndex, int length)			yes	no
writeBytes(ChannelBuffer src, int length)				yes	yes
writeBytes(ChannelBuffer src)						yes	yes

setBytes(int index, ChannelBuffer src, int srcIndex, int length)	no	no			
setBytes(int index, ChannelBuffer src, int length)			no	yes
setBytes(int index, ChannelBuffer src)					no	yes

							改变自己的readerIndex	改变参数的readerIndex
getBytes(int index, ChannelBuffer dst, int dstIndex, int length)	no	no
getBytes(int index, ChannelBuffer dst, int length)			no	yes
getBytes(int index, ChannelBuffer dst)					no	yes

readBytes(ChannelBuffer dst, int dstIndex, int length)			yes	no
readBytes(ChannelBuffer dst, int length)				yes	yes
readBytes(ChannelBuffer dst)						yes	yes
================ChannelBuffer=========================================================================================

[[[[[[[[[[[[[[[[[[[[[[[WrappedChannelBuffer[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[

SlicedChannelBuffer
	private final ChannelBuffer buffer;
	private final int adjustment;
	private final int length;
	public SlicedChannelBuffer(ChannelBuffer buffer, int index, int length) {
		if (index < 0 || index > buffer.capacity()) {
		    throw new IndexOutOfBoundsException("Invalid index of " + index
			    + ", maximum is " + buffer.capacity());
		}

		if (index + length > buffer.capacity()) {
		    throw new IndexOutOfBoundsException("Invalid combined index of "
			    + (index + length) + ", maximum is " + buffer.capacity());
		}

		this.buffer = buffer;
		adjustment = index;
		this.length = length;
		writerIndex(length);
	}

TruncatedChannelBuffer
	private final ChannelBuffer buffer;
	private final int length;
	public TruncatedChannelBuffer(ChannelBuffer buffer, int length) {
		if (length > buffer.capacity()) {
		    throw new IndexOutOfBoundsException("Length is too large, got "
			    + length + " but can't go higher than " + buffer.capacity());
		}

		this.buffer = buffer;
		this.length = length;
		writerIndex(length);
	}

DuplicatedChannelBuffer	
	private final ChannelBuffer buffer;
	public DuplicatedChannelBuffer(ChannelBuffer buffer) {
		if (buffer == null) {
		    throw new NullPointerException("buffer");
		}
		this.buffer = buffer;
		setIndex(buffer.readerIndex(), buffer.writerIndex());
	}

总结：
DuplicatedChannelBuffer、TruncatedChannelBuffer、SlicedChannelBuffer都与buffer共享底层存储如数组等；
	readerIndex、writerIndex都是独立的。
DuplicatedChannelBuffer的readerIndex、writerIndex与buffer的值相同。
TruncatedChannelBuffer、SlicedChannelBuffer都是readerIndex == 0；writerIndex == length。
TruncatedChannelBuffer与SlicedChannelBuffer相比，没有adjustment，（可以看做adjustment==0）

TruncatedChannelBuffer(buffer,length) 《=》 SlicedChannelBuffer(buffer,0,length)

DuplicatedChannelBuffer是复制一个ChannelBuffer，readerIndex、writerIndex独立，但值相同；共享。
TruncatedChannelBuffer

duplicate()	《=》 buf.slice(0, buf.capacity())
slice()		《=》 buf.slice(buf.readerIndex(), buf.readableBytes())

copy()		《=》 buf.copy(buf.readerIndex(), buf.readableBytes())	（copy结果与buffer不共享底层存储）


===================为何用input.slice()得到一套独立的readerIndex、writerIndex？==============================
FrameDecoder.messageReceived(ChannelHandlerContext ctx, MessageEvent e)中：
	try {
            callDecode(ctx, e.getChannel(), input, e.getRemoteAddress());
        } finally {
            updateCumulation(ctx, input);
        }
在updateCumulation(ctx, input)中：
	cumulation = newCumulation = input.slice();
问题：为何用input.slice()得到一套独立的readerIndex、writerIndex？
原因：在callDecode(ctx, e.getChannel(), input, e.getRemoteAddress());中：
	Object frame = decode(context, channel, cumulation);
返回的frame可能包含cumulation（就是input），后续的处理过程会对其读写操作；而这里的cumulation将input剩余的部分保存起来，后面也会有读写操作，两者冲突。
======================================================================================================================


]]]]]]]]]]]]]]]]]]]]]]]]]]WrappedChannelBuffer]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]


===============problems============= 
client channel close后，server channel 要不要手动close？？？

2013-1-2 21:21:25：
server在read得到-1（表示client的channel close了？！）、write抛出IOException后，都会close(channel, succeededFuture(channel));
=============================================================

NioWorkerPool父类AbstractNioWorkerPool构造函数中，workers = new AbstractNioWorker[workerCount]，init（）中workers[i] = newWorker(workerExecutor)，（new NioWorker(executor, determiner)），NioWorker的父类AbstractNioWorker的父类AbstractNioSelector构造函数中，openSelector(determiner)，openSelector(ThreadNameDeterminer determiner)中selector = Selector.open();DeadLockProofWorker.start(executor, newThreadRenamingRunnable(id, determiner));，AbstractNioWorker将自己包装一下：new ThreadRenamingRunnable(this, "New I/O worker #" + id, determiner);，AbstractNioSelector中的run开始运行。


NioServerBossPool(Executor bossExecutor, int bossCount, ThreadNameDeterminer determiner)
	AbstractNioBossPool(Executor bossExecutor, int bossCount)：bosses = new Boss[bossCount];
		init()：bosses[i] = newBoss(bossExecutor);
			newBoss(Executor executor)：new NioServerBoss(executor, determiner);
NioServerBoss(Executor bossExecutor, ThreadNameDeterminer determiner)
	AbstractNioSelector(Executor executor, ThreadNameDeterminer determiner)（同上）


	0		1		2
channelOpen =》 channelBound =》 channelConnected
						-1
channelClosed 《= channelUnbound 《=  channelDisconnected

=========================================
channelOpen：	futureQueue.offer(evt.getChannel().bind(localAddress));
	bind(SocketAddress localAddress)
		Channels.bind(this, localAddress);
			 channel.getPipeline().sendDownstream（。。。。）
				DefaultChannelHandlerContext是null，所以getSink().eventSunk(this, e)
					handleServerSocket(e);
						((NioServerBoss) channel.boss).bind(channel, future, (SocketAddress) value);
							registerTask(new RegisterTask(channel, future, localAddress));
								taskQueue.add(task);
								if (wakenUp.compareAndSet(false, true)) {
									selector.wakeup();
								}


task run():
	channel.socket.socket().bind(localAddress, channel.getConfig().getBacklog());
	future.setSuccess();
	fireChannelBound(channel, channel.getLocalAddress());
	channel.socket.register(selector, SelectionKey.OP_ACCEPT, channel);
	
						
NioServerBoss的run（）：（AbstractNioSelector，NioClientBoss和NioServerBoss的run都直继承它的run；AbstractNioWorker的run重写后super.run()，NioDatagramWorker和NioWorker的run都再次重写后super.run()）
	for(;;){
		selector.select(500)
		if (wakenUp.get()) {
		    selector.wakeup();
		} 
		processTaskQueue();
		if (shutdown) {
		。。。
		} else {
		    process(selector);
		}	
	}

ps：selector检测到可以accept的那啥后：
NioServerBoss.process(selector)：
	for (Iterator<SelectionKey> i = selectedKeys.iterator(); i.hasNext();) {
            SelectionKey k = i.next();
            i.remove();
            NioServerSocketChannel channel = (NioServerSocketChannel) k.attachment();

            try {
                for (;;) {
                    SocketChannel acceptedSocket = channel.socket.accept();
                    if (acceptedSocket == null) {
                        break;
                    }
                    registerAcceptedChannel(channel, acceptedSocket, thread);
                }
            }catch。。。。。（）{
	    
	    }

NioServerBoss.registerAcceptedChannel(NioServerSocketChannel parent, SocketChannel acceptedSocket, Thread currentThread)：
	try {
            ChannelSink sink = parent.getPipeline().getSink();
            ChannelPipeline pipeline =
                    parent.getConfig().getPipelineFactory().getPipeline();
            NioWorker worker = parent.workerPool.nextWorker();
            worker.register(new NioAcceptedSocketChannel(
                    parent.getFactory(), pipeline, parent, sink
                    , acceptedSocket,
                    worker, currentThread), null);
        } catch。。。

AbstractNioSelector.register(Channel channel, ChannelFuture future):
	1: Runnable task = createRegisterTask(channel, future);
        2: registerTask(task);
1:
NioWorker.createRegisterTask(Channel channel, ChannelFuture future):
	boolean server = !(channel instanceof NioClientSocketChannel);
        return new RegisterTask((NioSocketChannel) channel, future, server);

NioWorker.RegisterTask.run()：（register selector）
	if (localAddress == null || remoteAddress == null) {
		close(channel, succeededFuture(channel));
		return;
	}
	if (server) {
	    channel.channel.configureBlocking(false);
	}
	channel.channel.register(selector, channel.getRawInterestOps(), channel);
	......
	fireChannelConnected(channel, remoteAddress);

插入：
	NioWorker.run():
		super.run();
		recvBufferPool.releaseExternalResources();
	AbstractNioWorker.run():（就是上面的super.run()）
		super.run();
		sendBufferPool.releaseExternalResources();
	AbstractNioSelector.run():（就是上面的super.run()）
		（上文中，NioServerBoss的run（）已经描述过。这里面，调用上面的NioWorker.RegisterTask.run()，和下面的process）

	AbstractNioWorker.process(Selector selector):
==============process很大=========
遍历selector.selectedKeys()中的SelectionKey：

操作类型：int readyOps = k.readyOps();

读：if ((readyOps & SelectionKey.OP_READ) != 0 || readyOps == 0) ：
NioWorker.read(SelectionKey k):
	读取数据，用的buffer是DirectByteBuffer，重用，size是1024的倍数，size不满足等情况，先销毁再创建新的，销毁用反射，因为DirectByteBuffer和其中的cleaner是不可见的。（class.forName（）、method.setAccessible(true)）；读完后，复制到BigEndianHeapChannelBuffer中，最后fireMessageReceived(channel, buffer)。

	final SocketChannel ch = (SocketChannel) k.channel();这是NioAcceptedSocketChannel中的channel，由SocketChannel			acceptedSocket = channel.socket.accept();而来。
        final NioSocketChannel channel = (NioSocketChannel) k.attachment();这是NioAcceptedSocketChannel自己。

	读操作结束（ret = ch.read(bb)后ret<0（注：ret == 0是没读到数据，不会关闭；ret == -1是channel关闭））或出现异常，就关闭NioAcceptedSocketChannel：
		k.cancel(); // Some JDK implementations run into an infinite loop without this.
		close(channel, succeededFuture(channel));
		return false;

写：if ((readyOps & SelectionKey.OP_WRITE) != 0)：
AbstractNioWorker.writeFromSelectorLoop(final SelectionKey k):


=============================================
	
2:
AbstractNioSelector.registerTask(Runnable task):
	taskQueue.add(task);

        Selector selector = this.selector;

        if (selector != null) {
            if (wakenUp.compareAndSet(false, true)) {
                selector.wakeup();
            }
        } else {
            if (taskQueue.remove(task)) {
                // the selector was null this means the Worker has already been shutdown.
                throw new RejectedExecutionException("Worker has already been shutdown");
            }
        }

======================
client发请求、server返回结果的write过程：

AbstractChannel.write(Object message)：
	return Channels.write(this, message);
Channels.write(Channel channel, Object message):
	return write(channel, message, null);
Channels.write(Channel channel, Object message, SocketAddress remoteAddress):
	ChannelFuture future = future(channel);
        channel.getPipeline().sendDownstream(new DownstreamMessageEvent(channel, future, message, remoteAddress));
        return future;
DefaultChannelPipleine.sendDownstream(ChannelEvent e):
	DefaultChannelHandlerContext tail = getActualDownstreamContext(this.tail);
        if (tail == null) {
            try {
                getSink().eventSunk(this, e);
                return;
            } catch (Throwable t) {
                .....
            }
        }
        sendDownstream(tail, e);
DefaultChannelPipleine.sendDownstream(DefaultChannelHandlerContext ctx, ChannelEvent e):
	((ChannelDownstreamHandler) ctx.getHandler()).handleDownstream(ctx, e);
SimpleChannelDownstreamHandler.handleDownstream(ChannelHandlerContext ctx, ChannelEvent e):
	if (e instanceof MessageEvent) {
            writeRequested(ctx, (MessageEvent) e);
        }else.......
SimpleChannelDownstreamHandler.writeRequested(ChannelHandlerContext ctx, MessageEvent e):
	自定义处理过程。
	注意，这里不能将MessageEvent吞掉，而是最后要继续发给下一个ChannelHandlerContext（否则，MessageEvent到此为止，发不出去）。
	几种方式：Channels.write(ctx, e.getFuture(), cb, e.getRemoteAddress());（私：重新new了一个MessageEvent，因为一般情况下			，MessageEvent中的message Object发生了变化。）
		或ctx.sendDownstream(e);

上面的继续发给下一个ChannelHandlerContext，下一个为null，则：
NioServerSocketPipelineSink.eventSunk( ChannelPipeline pipeline, ChannelEvent e):
	else if (channel instanceof NioSocketChannel) {
            handleAcceptedSocket(e);
        }
NioServerSocketPipelineSink.handleAcceptedSocket(ChannelEvent e):
	if (e instanceof ChannelStateEvent) {
		......
	}
	else if (e instanceof MessageEvent) {
            MessageEvent event = (MessageEvent) e;
            NioSocketChannel channel = (NioSocketChannel) event.getChannel();
            boolean offered = channel.writeBufferQueue.offer(event);
            assert offered;
            channel.worker.writeFromUserCode(channel);
        }
	


=========================================================================
SocketSendBuffer.poolHead：write过程可能创建多个new Preallocation(DEFAULT_PREALLOCATION_SIZE)，PooledSendBuffer在release的时候，如果所包含的Preallocation parent中的int refCnt为0，则parent.buffer.clear();，如果parent != current 则poolHead = new PreallocationRef(parent, poolHead);，也就是poolHead指向parent，并且其next为之前的poolHead。


===========================================================================================================================
write将message中的ChannelBuffer复制到DirectBuffer，再将其写入WritableByteChannel（这个写入需要ByteBuffer参数，这里用DirectBuffer，没用ChannelBuffer的toByteBuffer方法将其复制到普通的ByteBuffer。）。每个AbstractNioWorker都对DirectBuffer有重用机制。

Non-direct与direct ByteBuffer区别：http://blog.csdn.net/lovehota/article/details/7407574
采用Non-directByteBuffer的流程是这样的：
 网络 --> 临时的DirectByteBuffer --> 应用 Non-directByteBuffer --> 临时的DirectByteBuffer --> 网络

而采用DirectByteBuffer的流程是这样的：
网络 --> 应用 DirectByteBuffer --> 网络
==================================================================================================================

[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[
注意：64 * 1024 == 65536:

DefaultNioSocketChannelConfig:
	private volatile int writeBufferHighWaterMark = 64 * 1024;

AbstractNioWorker：
	protected final SocketSendBufferPool sendBufferPool = new SocketSendBufferPool();
SocketSendBufferPool:
	private static final int DEFAULT_PREALLOCATION_SIZE = 65536;
	private Preallocation current = new Preallocation(DEFAULT_PREALLOCATION_SIZE);
Preallocation:
	final ByteBuffer buffer;
        int refCnt;
        Preallocation(int capacity) {
            buffer = ByteBuffer.allocateDirect(capacity);
        }

AbstractNioChannel:
	MessageEvent currentWriteEvent;
	SendBuffer currentWriteBuffer;（SendBuffer与SocketSendBufferPool的Preallocation共享底层的ByteBuffer）


]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]

[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[
注意：
AbstractChannel:
	public boolean isReadable() {
		return (getInterestOps() & OP_READ) != 0;
	}

	public boolean isWritable() {
		return (getInterestOps() & OP_WRITE) == 0;
	}
]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]